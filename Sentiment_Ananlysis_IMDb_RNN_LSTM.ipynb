{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of RNN_LSTM_IMDb_Dataset.ipynb","provenance":[{"file_id":"1qQ0PJZWOUdThEbxR7JRH8Wt0oxHwJ2UQ","timestamp":1594828746089}],"collapsed_sections":[],"authorship_tag":"ABX9TyPA1xvHHc3RBRTQShZmQ0BK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"82Hlvq4Yvhie","colab_type":"text"},"source":["**Problem Statement 2**\n","\n","Classify movie reviews as positive or negative using the text of the review."]},{"cell_type":"markdown","metadata":{"id":"Ou-o7TFSwqVZ","colab_type":"text"},"source":["Source -The IMDB dataset is available on imdb reviews or on TensorFlow datasets. "]},{"cell_type":"code","metadata":{"id":"335TSTsRvbpy","colab_type":"code","colab":{}},"source":["import time\n","start_time = time.time();\n","\n","# Importing libraries\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense\n","from keras.layers.embeddings import Embedding\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kAcLKKrrwnXJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594827651192,"user_tz":-330,"elapsed":4015,"user":{"displayName":"Varad Nerlekar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVHC7Um938o8f14Ez2u8iLJQx9ASIMzPsqPbKrEA=s64","userId":"02049849883426406134"}}},"source":["# Importing IMDb Datasets directly from keras datasets\n","from keras.datasets import imdb"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"msWM8HClyH7T","colab_type":"code","colab":{}},"source":["# Loading the data into train and test sets\n","max_num = 10000;\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_num)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T7DJQ2CMyw-W","colab_type":"code","colab":{}},"source":["# Plotting no. of words for padding\n","import seaborn as sns\n","\n","print(x_train)\n","\n","word_length = [len(i) for i in x_train]\n","sns.distplot(word_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0tDsSsb_LEp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594828707184,"user_tz":-330,"elapsed":912,"user":{"displayName":"Varad Nerlekar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVHC7Um938o8f14Ez2u8iLJQx9ASIMzPsqPbKrEA=s64","userId":"02049849883426406134"}},"outputId":"cc0dc691-6176-43ee-e182-6b1cdafc11ba"},"source":["# Padding data\n","padding_num = 100;\n","x_train = pad_sequences(x_train, maxlen = padding_num)\n","x_test = pad_sequences(x_test, maxlen = padding_num)\n","print(x_train.shape)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["(25000, 100)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5WWMftEM7Nyb","colab_type":"code","colab":{}},"source":["# LSTM Network\n","word_size = 10000\n","embed_size = 128\n","\n","model = Sequential()\n","model.add(Embedding(word_size, embed_size, input_shape = (x_train.shape[1],)))\n","model.add(LSTM(units=50, activation = 'tanh'))\n","model.add(Dense(units=1, activation = 'sigmoid'))\n","model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxAQFj0B92J2","colab_type":"code","colab":{}},"source":["# Training the Model\n","history = model.fit(x_train, y_train, validation_data = (x_test, y_test),\n","                    epochs = 5, batch_size=50, verbose = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fQOVX2SCCN1w","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# List all data in history\n","print(history.history.keys())\n","\n","# Summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","\n","# Summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","\n","scores = model.evaluate(x_test, y_test, verbose=1)\n","\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n","print(); print(\"Execution Time %s seconds: \" % (time.time() - start_time))"],"execution_count":null,"outputs":[]}]}